{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the dataset folder\n",
    "train_set_folder = 'own_dataset'\n",
    "output_folder = 'preprocessed'\n",
    "output_folder_2 = 'augmented'\n",
    "def create_directory_if_not_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Ensure output folders exist\n",
    "create_directory_if_not_exists(output_folder)\n",
    "create_directory_if_not_exists(output_folder_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import DepthwiseConv2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Custom DepthwiseConv2D layer\n",
    "class CustomDepthwiseConv2D(DepthwiseConv2D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if 'groups' in kwargs:\n",
    "            kwargs.pop('groups')\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "# Check if image is valid\n",
    "def check_image_integrity(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img.verify()  # Verify that it is an image\n",
    "        return True\n",
    "    except (IOError, SyntaxError) as e:\n",
    "        print(f\"Corrupted image: {img_path} - {e}\")\n",
    "        return False\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocess_image(img):\n",
    "    img_size = (300, 300)  # Resize images to 300x300 pixels\n",
    "    img_resized = cv2.resize(img, img_size)\n",
    "    gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)\n",
    "    eq_img = cv2.equalizeHist(gray)\n",
    "    return eq_img\n",
    "\n",
    "# Function to process and save images\n",
    "def preprocess_and_save(input_folder, output_folder):\n",
    "    create_directory_if_not_exists(output_folder)\n",
    "    \n",
    "    for label in os.listdir(input_folder):\n",
    "        label_path = os.path.join(input_folder, label)\n",
    "        print(\"here2\", label_path)\n",
    "\n",
    "        \n",
    "        if os.path.isdir(label_path):\n",
    "            output_label_path = os.path.join(output_folder, label)\n",
    "            create_directory_if_not_exists(output_label_path)\n",
    "            \n",
    "            for img_name in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, img_name)\n",
    "                if os.path.isfile(img_path) and check_image_integrity(img_path):\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is not None:\n",
    "                        processed_img = preprocess_image(img)\n",
    "                        save_path = os.path.join(output_label_path, img_name)\n",
    "                        cv2.imwrite(save_path, processed_img)\n",
    "                    else:\n",
    "                        print(f\"Failed to load image: {img_path}\")\n",
    "\n",
    "# Run preprocessing\n",
    "preprocess_and_save(train_set_folder, output_folder)\n",
    "print(\"Preprocessing completed. Images saved to:\", output_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "def augment_and_save(input_folder, output_folder_2, augment_count=5):\n",
    "    create_directory_if_not_exists(output_folder_2)\n",
    "    \n",
    "    for label in os.listdir(input_folder):\n",
    "        label_path = os.path.join(input_folder, label)\n",
    "        \n",
    "        if os.path.isdir(label_path):\n",
    "            output_label_path = os.path.join(output_folder_2, label)\n",
    "            create_directory_if_not_exists(output_label_path)\n",
    "            \n",
    "            for img_name in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, img_name)\n",
    "                if os.path.isfile(img_path):\n",
    "                    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is not None:\n",
    "                        img = cv2.resize(img, (300, 300))  # Resize to 300x300\n",
    "                        img = img.reshape((1,) + img.shape + (1,))\n",
    "                        \n",
    "                        for i, batch in enumerate(datagen.flow(img, batch_size=1)):\n",
    "                            augmented_img = batch[0].astype('uint8')\n",
    "                            save_path = os.path.join(output_label_path, f\"{os.path.splitext(img_name)[0]}_aug_{i}.jpg\")\n",
    "                            cv2.imwrite(save_path, augmented_img)\n",
    "                            \n",
    "                            if i >= augment_count - 1:\n",
    "                                break\n",
    "                    else:\n",
    "                        print(f\"Failed to load image: {img_path}\")\n",
    "\n",
    "# Run augmentation\n",
    "augment_and_save(output_folder, output_folder_2, augment_count=5)\n",
    "print(\"Data augmentation completed. Augmented images saved to:\", output_folder_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Function to load image paths and labels\n",
    "def load_images_and_labels(dataset_folder):\n",
    "    X, y = [], []\n",
    "    for label in os.listdir(dataset_folder):\n",
    "        label_path = os.path.join(dataset_folder, label)\n",
    "        if os.path.isdir(label_path):\n",
    "            for img_name in os.listdir(label_path):\n",
    "                img_path = os.path.join(label_path, img_name)\n",
    "                if os.path.isfile(img_path):\n",
    "                    X.append(img_path)\n",
    "                    y.append(label)\n",
    "                else:\n",
    "                    print(f\"Skipping non-file: {img_path}\")\n",
    "        else:\n",
    "            print(f\"Skipping non-directory: {label_path}\")\n",
    "    print(f\"Loaded {len(X)} image paths and {len(y)} labels.\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Generator function to load images in batches\n",
    "def image_generator(X, y, batch_size=32):\n",
    "    num_samples = len(X)\n",
    "    while True:\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_paths = X[offset:offset+batch_size]\n",
    "            batch_labels = y[offset:offset+batch_size]\n",
    "            \n",
    "            batch_images = []\n",
    "            for img_path in batch_paths:\n",
    "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "                if img is not None:\n",
    "                    img = cv2.resize(img, (300, 300))\n",
    "                    img = img / 255.0  # Normalize\n",
    "                    batch_images.append(img)\n",
    "                else:\n",
    "                    print(f\"Failed to load image: {img_path}\")\n",
    "            \n",
    "            batch_images = np.array(batch_images).reshape(-1, 300, 300, 1)\n",
    "            yield batch_images, batch_labels\n",
    "\n",
    "# Load image paths and labels\n",
    "X, y = load_images_and_labels(output_folder_2)\n",
    "\n",
    "# Check if data was loaded correctly\n",
    "if len(X) == 0 or len(y) == 0:\n",
    "    raise ValueError(\"No images were loaded. Please check the dataset path and structure.\")\n",
    "\n",
    "# Encode all labels\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split data into training (70%), validation (15%), and test (15%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_encoded, test_size=0.3, stratify=y_encoded, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "print(\"Data splitting completed.\")\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_val)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Create generators\n",
    "train_gen = image_generator(X_train, y_train)\n",
    "val_gen = image_generator(X_val, y_val)\n",
    "test_gen = image_generator(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "# Define the model building function for Keras Tuner\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=32),\n",
    "                            kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),\n",
    "                            activation='relu',\n",
    "                            input_shape=(300, 300, 1)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(filters=hp.Int('conv_2_filter', min_value=64, max_value=256, step=64),\n",
    "                            kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]),\n",
    "                            activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    \n",
    "    model.add(layers.Conv2D(filters=hp.Int('conv_3_filter', min_value=128, max_value=512, step=128),\n",
    "                            kernel_size=hp.Choice('conv_3_kernel', values=[3, 5]),\n",
    "                            activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    model.add(layers.Dense(units=hp.Int('dense_1_units', min_value=128, max_value=512, step=32),\n",
    "                           activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    model.add(layers.Dense(27, activation='softmax'))  # 27 classes: 26 alphabets + space\n",
    "    \n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Set up the tuner\n",
    "tuner = RandomSearch(build_model,\n",
    "                     objective='val_accuracy',\n",
    "                     max_trials=10,\n",
    "                     executions_per_trial=2,\n",
    "                     directory='output_dir',\n",
    "                     project_name='sign_language_model')\n",
    "\n",
    "# Run the tuner search\n",
    "tuner.search(train_gen,\n",
    "             steps_per_epoch=len(X_train) // 32,  # Changed back to 32 to match the generator\n",
    "             epochs=50,\n",
    "             validation_data=val_gen,\n",
    "             validation_steps=len(X_val) // 32,  # Changed back to 32\n",
    "             callbacks=[EarlyStopping(patience=5), ReduceLROnPlateau()])\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_acc = best_model.evaluate(test_gen, steps=len(X_test) // 32)\n",
    "print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# Save the model\n",
    "best_model.save('sign_language_model.h5')\n",
    "print(\"Model training and evaluation completed. Model saved.\")\n",
    "\n",
    "# Save the label encoder classes\n",
    "np.save('classes.npy', label_encoder.classes_)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
